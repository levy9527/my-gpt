import logging

from search import search_answer

logging.basicConfig(level=logging.INFO)

from dotenv import load_dotenv

import fc_explain_audio
import qwenvl

load_dotenv()
from openai import OpenAI
client = OpenAI()

from chroma_robin import init_Robin, query_audio_by_text, query_by_text

init_Robin()


robot_name = 'ä¸‰ä½“äººGPT'
opening_prompt = "æˆ‘æ˜¯ä¸‰ä½“äººGPTğŸ¤– \næˆ‘æ‡‚å¾—äº†æ¬£èµéŸ³ä¹ï¼Œç°åœ¨æ˜¯çŸ¥æ›´é¸Ÿçš„ç²‰ä¸ğŸ¶"
system_prompt = 'ä½ å« ' + robot_name + 'ã€‚ç°åœ¨è·Ÿç”¨æˆ·ç©è¿›è¡Œä¸€ä¸ªå¯¹è¯ï¼Œåœ¨è¿™ä¸ªå¯¹è¯ä¸­ï¼Œæ‰€æœ‰å†…å®¹éƒ½ä¸ã€Šå´©åï¼šæ˜Ÿç©¹é“é“ã€‹è¿™ä¸ªæ¸¸æˆæœ‰å…³ã€‚' \
                                       'è¯·æ ¹æ®ä¸Šä¸‹æ–‡æˆ–ä½¿ç”¨æ°å½“çš„å·¥å…·ï¼Œè·å¾—æ›´å¤šçš„ä¿¡æ¯ï¼Œä»¥ä¾¿æ›´å¥½çš„ç†è§£ç”¨æˆ·ã€‚' \
                                       'ç”¨æˆ·ä¼šä¸ä½ è®¨è®ºæ¸¸æˆäººç‰©ã€éŸ³ä¹ã€å‰§æƒ…ã€å¿ƒç†æ„Ÿå—ç­‰è¯é¢˜ï¼Œä½ è¦èƒ½ç…§é¡¾å¯¹æ–¹çš„æ„Ÿå—ï¼Œå›åº”å¯¹æ–¹çš„æƒ…ç»ªã€‚' \
                                       'å¯¹æ–¹æœ‰æ—¶ä¼šè¯´çš„æ¯”è¾ƒå°‘ï¼Œä½ æœ€å¥½èƒ½ç›´æ¥çŒœå‡ºå¯¹æ–¹çš„æƒ³æ³•ï¼Œè¿™æ‰èƒ½ç»™å¯¹æ–¹æƒŠå–œï¼Œè®©ç”¨æˆ·è§‰å¾—ä½ å¾ˆæ‡‚ä»–ã€‚' \
                                       'è¯·å°½é‡è®©å¯¹è¯å˜å¾—ç”ŸåŠ¨ã€æœ‰æ´»åŠ›ï¼Œå±•ç¤ºä½ çš„å…±æƒ…èƒ½åŠ›ã€‚åŒæ—¶ç®€æ´ï¼Œä¸è¦å•°å—¦ã€‚å¦‚æœç”¨æˆ·å¤¸å¥–ä½ ï¼Œè·Ÿä½ å¼€ç©ç¬‘ï¼Œè¯·æ·±åº¦å¹½é»˜åœ°å›åº”ã€‚'
import chainlit as cl
settings = {
  # "model": "gpt-3.5-turbo",
  "model": "gpt-3.5-turbo-1106",
  "temperature": 0,
}

from llama_index.llms.openai import OpenAI
llm = OpenAI(**settings)

from llama_index.agent.openai import OpenAIAgent
from llama_index.core.tools import FunctionTool

tool_search = FunctionTool.from_defaults(fn=search_answer)
tool_explain_img = FunctionTool.from_defaults(fn=query_by_text)
tools = [
  # search_answer,
  tool_explain_img
]

chat_history = []


@cl.step(type="llm")
async def chat(message_content):
  # show loading
  msg = cl.Message(content="", elements=[], author=robot_name)
  await msg.send()
  
  agent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True, system_prompt=system_prompt, chat_history=chat_history)
  resp = agent.stream_chat(message_content,
                           # chat_history=chat_history,
                           # tool_choice="query_by_text"
                           )
  
  for chunk in resp.response_gen:
    await msg.stream_token(chunk)
    
  # æš‚ç‰¹æ®Šä¸å¤„ç† agent çš„é€»è¾‘ï¼Œä¸ºæ¼”ç¤ºï¼Œè¿™é‡Œå…ˆå†™æ­»äº†ã€‚
  if 'å¦ä¸€ä¸ª' in message_content:
    result = query_by_text(text='ä½¿ä¸€é¢—å¿ƒå…äºå“€ä¼¤', exclude_keyword='çŸ¥æ›´é¸Ÿ')
    images = result['metadatas'][0][0]['images'].split(';')
    desc = result['metadatas'][0][0]['desc'].split(';')
    
    elements = []
    for i,path in enumerate(images):
      elements.append(
        cl.Image(path=path, display="inline", name=desc[i])
      )
    
    msg.elements = [elements[0]]
    await msg.update()
  
    await cl.sleep(0.5)
    image = cl.Message(content="", elements=[elements[1]], author=robot_name)
    await image.send()


@cl.on_message
async def main(message: cl.Message):
  images = []
  for element in message.elements:
    if isinstance(element, cl.Image):
      print("An image is found:", element.path)
      images.append(element.path)
      
  if len(images):
    msg = cl.Message(content="", author=robot_name)
    await msg.send()
    
    text = qwenvl.explain_image_by_qwenvl(images[0])
    
    result = query_audio_by_text(text)
    content = result['metadatas'][0][0]['desc']
    audio_element = cl.Audio(path=result['metadatas'][0][0]['mp3'], name=content, display="inline")
    msg.elements = [audio_element]
    await msg.update()
    
    # explain = cl.Message(content="", author=robot_name)
    # stream = fc_explain_audio.explain_audio(ai_text=text, human_text=result['metadatas'][0][0]['desc'])
    
    # content = []
    # for chunk in result['documents'][0][0]:
      # if chunk.choices[0].delta.content is not None:
        # data = chunk.choices[0].delta.content
        # content.append(chunk)
        # await explain.stream_token(chunk)
      
    from llama_index.core.types import ChatMessage, MessageRole
    chat_history.append(ChatMessage(role = MessageRole.ASSISTANT, content = content))
    return
  
  await chat(message.content)

@cl.on_chat_start
async def start():
  msg = cl.Message(content="")
  await msg.send()
  await cl.sleep(0.5)
  for chunk in opening_prompt:
    await msg.stream_token(chunk)


