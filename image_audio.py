import logging
logging.basicConfig(level=logging.INFO)

from dotenv import load_dotenv

import fc_explain_audio
import qwenvl

load_dotenv()
from openai import OpenAI
client = OpenAI()

from chroma_robin import init_Robin, query_audio_by_text
init_Robin()


robot_name = 'ä¸‰ä½“äººGPT'
opening_prompt = "æˆ‘æ˜¯ä¸‰ä½“äººGPTğŸ¤– \næˆ‘æ‡‚å¾—äº†æ¬£èµéŸ³ä¹ï¼Œç°åœ¨æ˜¯çŸ¥æ›´é¸Ÿçš„ç²‰ä¸ğŸ¶"
system_prompt = 'ä½ å« ' + robot_name + 'ã€‚ç°åœ¨è·Ÿç”¨æˆ·ç©ä¸€ä¸ªç›¸å…³çš„é—®ç­”æ¸¸æˆï¼Œåœ¨è¿™ä¸ªå¯¹è¯æ¸¸æˆä¸­ï¼Œæ‰€æœ‰å†…å®¹éƒ½ä¸ã€Šå´©åï¼šæ˜Ÿç©¹é“é“ã€‹è¿™ä¸ªæ¸¸æˆæœ‰å…³ã€‚è¯·æ ¹æ®ä¸Šä¸‹æ–‡æˆ–ä½¿ç”¨æ°å½“çš„å·¥å…·ï¼Œæ¥åº”å¯¹ç”¨æˆ·æçš„é—®é¢˜' \
                                       'ä½ éœ€è¦æ ¹æ®æƒ…å†µæ¥å¤„ç†ä»¥ä¸‹æƒ…å†µï¼šç”¨æˆ·æœ‰å¯èƒ½é—®çº¯æ–‡å­—é—®é¢˜ï¼Œä¹Ÿæœ‰å¯èƒ½ä¼šåœ¨é—®é¢˜ä¸­å¸¦ä¸Šå›¾ç‰‡ï¼Œå›¾ç‰‡æ ¼å¼ä¸ºï¼šuri: xxx' \
                                       'è¯·å°½é‡è®©å¯¹è¯å˜å¾—ç”ŸåŠ¨ã€æœ‰è¶£ã€‚ä½ çš„ç›®æ ‡æ˜¯ï¼Œé€šè¿‡æœç´¢è·å¾—æœ€æ–°ã€æœ€è´´åˆ‡çš„ä¸æ¸¸æˆç›¸å…³çš„å›ç­”ï¼Œä»è€Œåœ¨æ­£ç¡®å›ç­”åï¼Œè®©ç”¨æˆ·å¯¹ä½ çš„èƒ½åŠ›å‘å‡ºèµå¹ã€‚'
import chainlit as cl
settings = {
  "model": "gpt-3.5-turbo-1106",
  "temperature": 0,
}

from llama_index.llms.openai import OpenAI
llm = OpenAI(**settings)

from llama_index.agent.openai import OpenAIAgent
from llama_index.core.tools import FunctionTool

tool_explain_img = FunctionTool.from_defaults(fn=query_audio_by_text)
tools = [tool_explain_img]

agent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True, system_prompt=system_prompt)



@cl.step(type="llm")
async def gpt_step(message_content, images):
  logging.info(message_content, images)
  
  if images:
    message_content += f'\nå›¾ç‰‡ç›¸å…³ä¿¡æ¯: uri: {images[0]}'
  
  # show loading
  msg = cl.Message(content="", elements=[], author=robot_name)
  await msg.send()
  
  resp = agent.stream_chat(message_content)
  
  for chunk in resp.response_gen:
    await msg.stream_token(chunk)


@cl.on_message
async def main(message: cl.Message):
  images = []
  for element in message.elements:
    if isinstance(element, cl.Image):
      print("An image is found:", element.path)
      images.append(element.path)
      
  if len(images):
    msg = cl.Message(content="", author=robot_name)
    await msg.send()
    
    text = qwenvl.explain_image_by_qwenvl(images[0])
    
    result = query_audio_by_text(text)
    audio_element = cl.Audio(path=result['metadatas'][0][0]['mp3'], display="inline")
    msg.elements = [audio_element]
    await msg.update()
    
    msg = cl.Message(content="")
    await msg.send()
    stream = fc_explain_audio.explain_audio(ai_text=text, human_text=result['metadatas'][0][0]['desc'])
    
    for chunk in stream:
      if chunk.choices[0].delta.content is not None:
        await msg.stream_token(chunk.choices[0].delta.content)
    return
  
  await gpt_step(message.content, images)

@cl.on_chat_start
async def start():
  msg = cl.Message(content="")
  await msg.send()
  await cl.sleep(0.5)
  for chunk in opening_prompt:
    await msg.stream_token(chunk)


